---
title: "A pragmatic justification for predictive methods"
author:
  - name: Tim Huegerich
abstract: There is a pragmatic justification for expecting a predictive method to perform the same in the future as in the past. This justified expectation of continued predictive ability resolves Hume's problem of induction. It can also be used to assess scientific theories and provide a sound basis for statistical inference, though a full exploration of these broader applications is beyond the scope of this paper.

format: 
  html:
    toc: true
    embed-resources: true
theme: default
fig-cap-location: top
bibliography: references.bib
---

## Introduction

The aim of this paper is to provide a self-standing philosophical justification for continuing to use methods of prediction that have proven successful. That is, this justification of predictive methods is intended to be independent of claims about the confirmation of broader theories---or the truth of narrower inductive inferences---upon which predictions may otherwise be thought to derive from. Simply stated, to predict some unknown outcome, one should use a predictive method that has, or could have, been successful at predicting that outcome.

I use the word "prediction" broadly. As @forster2008 put it, "Suppose I choose a card and place it face down on the table. You have to predict whether I chose a diamond. Even though the event you are predicting happened in the past, we are comfortable with using the word 'predict,' as opposed to 'postdict.'" Predictions, so construed, are ubiquitous. They range from the everyday projections made automatically by our brains as we navigate the world to the sophisticated scientific predictions of particle physicists.

Many machine learning researchers already seem to regard successful predictions (under the rubric of cross-validation and related approaches) as a direct rationale for using a given model, so that embracing the argument here would have little or no impact on their practice. Others (such as most economists, in my experience) regard the use of cross-validation for model selection as an ad hoc approach that could only be justified in terms of more traditional statistics concepts, if at all. In the philosophy literature, while there are many close parallels, as discussed below, I am not aware of a precedent for directly justifying the use predictive methods, in themselves. Consistent with this sense is the following comment in a recent review article on "Philosophy of Statistics," in its section on model selection: "We leave aside methods that use cross-validation as they have, unduly, not received as much attention in the philosophical literature."[^1]

[^1]: [@romeijn2022].

The argument proceeds as follows. @sec-under shows how thinking in terms of "predictive methods" alleviates the problem of underdetermination. Given that, @sec-prag lays out the pragmatic justification for the case of deterministic predictions. @sec-prob extends the argument to probabilistic predictions. @sec-induct addresses the major arguments against Reichenbach's pragmatic justification of induction, which is closely related to this argument.[^2] @sec-schurz relates the framework of this paper to a prominent recent attempt to justify induction with reference to prediction.

[^2]: See [@henderson2022].

## Predictive methods are less underdetermined {#sec-under}

It would generally be difficult to fully specify the method used to make a given prediction, whether of the everyday or scientific variety. Even for an apparently basic prediction like "All ravens are black," to fully spell out how to determine whether something is a swan may be quite involved.[^3] In principle, though, we can identify a "predictive method," the reproducible procedure used to make a given prediction.

[^3]: Imagine trying to program a robot to do so. Current machine learning techniques could be used to train it to recognize a raven's telltale visual characteristics (aside from color) and sounds. It is somewhat more difficult to imagine automating the process a human would use to ascertain a bird's ancestry, whether by plucking a feather and analyzing its DNA or by somehow determining whether the bird could be traced back to known raven parents.

::: {#def-predictive}
A *predictive method* is a procedure for making predictions. It includes all concepts and tacit knowledge required to carry out the steps.
:::

Of course, the more common way to speak about "All ravens are black" is as a "hypothesis," a statement that is either true or false.[^4] By contrast, a predictive method does not have a truth value, per se, but rather a track record: have its predictions been successful? And in addition to its track record of literal predictions of then-unknown observations, one can also assess how well it *could have* predicted already-known data.[^5] In this "hold-out evaluation" approach, a predictive method is only allowed to use a subset of the available data, the "training set," as inputs for predicting the values of the held-out "test set."[^6]

[^4]: See [@dethier2018] for a broader discussion of the shortcomings of "the more sentence- or proposition-focused accounts of induction prevalent in twentieth-century debates." As an alternative, he cites [@forster2011] and others favoring William Whewell's approach to inductive inference, in which predictions play a key role.

[^5]: The former is sometimes called a "temporal prediction" in the literature on assessing scientific theories, with the latter referred to as a "heuristic" conception of predictions.[@barnes2022] Whether something is a heuristic prediction of a theory can be difficult to define. Eliminating the ambiguity between accommodation and prediction is one advantage of thinking in terms of predictive methods.

[^6]: Here I follow the terminology of [@hansen2021, p. 867]. He goes on to explain that it is sometimes "desirable to make a tripartite division of the sample into (1) training, (2) model selection, and (3) final estimation and assessment. This can be particularly useful when it is desired to obtain a parameter estimator whose distribution is not distorted by the model selection process."

Setting the ravens aside for a moment to more simply demonstrate the concept of hold-out evaluation, suppose our data consists of 10 exactly collinear (X, Y) points. Consider the predictive method of a least-mean-squared-error (LMSE) best-fit line. Given some input data, the procedure is to find the line that minimizes the sum of the squared differences between the known outcomes (Y) and the corresponding points on the line, then using that line to predict unknown outcomes for other values of X.[^7] Now, whether or not the best-fit-line predictive method was used to make any predictions prior to these 10 data points being known, it *could have* successfully predicted the Y values of any held-out evaluation set, as long as the training set contains at least two points.

[^7]: In some previous literature, it is a "family of curves" (or models) that is under evaluation. This LMSE best-fit line "predictive method" is analogous to the family of all straight lines, as defined in [@forster1994], for example. But a predictive method must also specify how the specific curve is chosen from among the family, given some input data. And the concept of a predictive method is also broader than that of a family of curves, encompassing the application of concepts and theories that cannot be represented as a family of curves, such as the ravens example.

To be merely a *hypothesis* consistent with the known observations is much lower bar than to be a successful predictive method. There are uncountably many functions consistent with those 10 observations, functions which vary wildly for X values other than those 10 points, generically speaking. But predictive methods corresponding to those nonlinear functions would not plausibly have been able to predict the Y values of held-out data. Predictive ability, even in just the hold-out evaluation sense, is sufficient to differentiate the line from the vast majority of alternatives in this case.

Still, we cannot necessarily conclude that the best-fit line is the unique predictive method capable of predicting any held-out subset of this data. Let's consider such potential alternative predictive methods in two categories:

1)  Those which, given all 10 data points as inputs, make exactly the same predictions for all unknown outcomes as the LMSE line.
2)  Those which differ in their predictions of some unknown values.

When the goal is making predictions, Case 1 is no problem. The same predictions are justified regardless. For example, the best-fit quadratic curve gives equivalent predictions in this example. There is no need to prefer one to the other if the purpose is the predictions themselves.[^8]

[^8]: [@forster2002] \[p. S131\] similarly points out that selection of a model with more parameters than the minimum necessary to represent the true curve is irrelevant in the limit as $N \to \infty$ "with respect to the goal of predictive accuracy*."*

Yet, what if there were a predictive method that could have plausibly predicted any held out point but differs from the best-fit line in its predictions for unknown points?

-   Add curve-fitting analog of grue, with discussion of grue in footnote.

For now, what of that particular form of Case 2 predictive method that, while unlikely to arise in practice, has some prominence within philosophical discussion of prediction in terms of hypotheses? I am referring to hypotheses that include words like "blite," defined as "black and first observed before the year 3000, or white and first observed after 3000."[^9] In terms of hypotheses, it has proven difficult to explain our natural preference for "All ravens are black" over the equally-consistent-with-the-data hypothesis "All ravens are blite." In terms of predictive methods, the riddle is that we seem to have two methods that are equally successful at predicting the color of ravens but that predict different things about the future---and similar "Case 2" predictive methods could be created for *any* predictive method.

[^9]: This follows the presentation in [@strevens2017, p. 64] of Nelson Goodman’s "new riddle of induction."

Spelled out as a predictive procedure, however, "All ravens are blite" means: First, determine whether something is a raven (as identified by characteristics other than color); second, check whether it is observed before the year 3000; predict that it will appear black if so, and white otherwise.[^10] Well, being prior to the year 3000, that second step is irrelevant for this procedure's track record. It has not really been part of the predictive method at all, properly speaking. Rather, the effective predictive method was simply determining whether something is a raven and, if so, predicting black as its color. Implicit in the definition of a predictive method, then, is a certain form of “Occam’s razor”: if a step or input in a predictive method can be identified as having no effect on any predictions it has (or could have) made, it should not be considered part of the predictive method that has or could have made those predictions.[^11]

[^10]: The color observation could be instrumentalized in terms of the RGB values of a digital photograph, say, something independent of one’s preference between the words “black” and “blite.”

[^11]: In the 10 linear points example above, this "razor" provides another way to handle the "Case 1" best-fit quadratic curve predictive method. More generally, though, there are situations in practice in which this "razor" does not strictly apply, particularly in the context of the probabilistic predictions we turn to in @sec-prob below. Then when we have two similarly successful predictive methods, the distinction between "Case 1" (both predict the same thing for the purpose at hand) and "Case 2" (the choice matters for the purpose at hand) will matter more.

It is difficult to imagine a legit Case 2 predictive method that would match the best-fit line in the example at hand, and that is the case for many other predictive methods. When thinking in terms of predictive methods rather than hypotheses, it is quite plausible that there be a uniquely successful predictive method for a given purpose. But there will also be situations in which equally successful predictive methods diverge in their predictions, and such situations will also be addressed in the following section.

## A justification for certain predictive methods {#sec-prag}

We never know for certain whether a successful predictive method will continue working, but there is a pragmatic justification for assuming past performance is informative of future success: otherwise, prediction is simply not possible. Prior predictive performance is our only source of information about whether a predictive method will work going forward. The basis for this claim is empiricism: our only possible way to judge whether a method works is whether it has (or could have) worked in the past.[^12] So we have nothing to lose by assuming that it is possible to use known information to predict the unknown.

[^12]: Suppose there were some other way of identifying a reliable predictive method: using a Ouija board, say. The only way we could assess whether the Ouija board reveals reliable predictive methods is to check whether the methods it dictates work.

A key feature of predictive methods here is that they are repeatable: their procedures can be repeated, checking the outcome each time. And to apply a predictive method is to reproduce the same procedure that has been (or could have been) found successful in the past. Past outcomes of predictive methods are of the same kind as intended future outcomes, commensurable.[^13] So there is a unique, natural generalization of past to future performance:[^14]

[^13]: By contrast, if the goal is arriving at the truth (or higher probability) of a hypothesis, an observation "confirming" the hypothesis is not the same kind of thing as the desired outcome.

[^14]: The investing disclaimer that “past performance is not indicative of future results” does not apply here in the same way (though it is true that future results are not guaranteed by a so-far successful predictive method). What is meant by “past performance” in the investment context is different from assessing the performance of a predictive method. On the contrary, research has shown that mutual funds’ past “performance” in the ROI sense is *not* predictive of future returns. That is, the investing disclaimer could be restated as “naively extrapolating past returns forward is a poor predictive method.”

::: {#prp-same}
It is reasonable to expect that a predictive method's future performance will match its past performance for similar predictions.
:::

What counts as "similar" predictions here may be considered to be part of the predictive method itself, without vicious circularity. A method characterizes its scope of applicability, and its past success is judged accordingly.

Implicit here, by the way, is a Baconian exhaustiveness in evaluating a predictive method's past success. One should not leap to conclusions based on a blinkered consideration of small number of successes without considering whether it would have worked in all other known instances, according to its defined scope of applicability.

Given @prp-same, we can make a stronger statement that using successful predictive methods is rationally compelled:

::: {#prp-worked}
A prediction should be made using a predictive method that has worked---or could have worked---to make similar predictions.
:::

Not only do we have nothing to lose by making predictions, but we have something to gain: predictions are necessary for purposeful action.[^15] That is, forming expectations of the outcomes of our actions---however provisional or implicit those expectations may be---is required. To have any basis for choosing one action over another, we need a way of using known information to predict the unknown outcomes, a predictive method. If one is available, and given @prp-same, we should use it.

[^15]: This argument follows Reichenbach's pragmatic vindication of induction (see [@henderson2022]), except that he attempted to justify the use of "straight rule" induction, which lacks the protection against underdetermination of validated predictive methods. See @sec-induct.

Again, I am not saying that we *know* a successful method will continue to work for future predictions. There are no such guarantees.[^16] And when a previously successful method fails, we should revise it. But unless or until that happens, we ought to use a predictive method that has worked.

[^16]: In this sense Hume's problem of induction remains unsolved and presumably unsolvable. See @sec-hume.

To clarify the nature of this claim, consider an example even simpler than a best-fit line. Imagine a child letting go of a spoon she was holding. Will it fall to the ground? Yes, it does. She tries again, with the same result. She surely should predict, going forward, that spoons will fall if she lets go of them. That is what I mean by “use what works.”

Thought of as a hypothesis, of course, the statement that a spoon always falls is false. An adult handing her a spoon tied to a large helium balloon will know that the result will be different. But until something like that happens, the child’s predictive method will have been useful. And even the final, doomed prediction involving the helium balloon is rationally justified. She should of course revise her predictive method after that failure (coming one step closer to the adult’s more sophisticated predictive method for the motion of spoons). But her “always falls” prediction, considered *a priori*, was sound---and even rationally compelled.

Another way to put @prp-worked is that we ought to *act as if there were* uniformity of nature in a specific sense: that successful predictive methods will continue to work. Otherwise prediction (and thus purposeful action) is impossible. One can choose to act in this way (or consciously assent in good faith to one’s natural inductive tendencies) despite acknowledging intellectually not knowing whether nature is reliably uniform in any sense. Again, there’s nothing to lose by acting in this way, and there is potentially something to gain. If past experience offers usable information, this approach will use it. If not, purposeful action was impossible anyway.

### How to distinguish insufficiently tested methods?

In this way of thinking, a greater quantity of successes, in itself, does not increase the justification for using a predictive method. And that may seem like a problem: how can it be the case that the rational warrant for using a predictive method known to work once is the same as for one used successfully thousands of times? Well, though a small number of instances is not a problem in itself, it is a problem to the extent the available evidence is not sufficient to distinguish alternative predictive methods.

If two equally successful predictive methods give different predictions about unknown data, *that* is the reason for caution before using either of them. This is the proper criterion for whether a predictive method has been "sufficiently tested." In such situations, there is no good reason to rely on one method rather than the other, and there is a special impetus to test the conflicting predictions in the hopes of establishing a uniquely successful predictive method than should then be used.[^17]

[^17]: This aspect of the approach proposed here is akin to the importance Karl Popper placed on scientists trying to falsify theories.

-   Clarify Popper relationship

## Choosing among uncertain predictive methods {#sec-prob}

The previous section addresses predictive methods with a perfect track record of making predictions---with no uncertainty. Often, however, the available methods for a given prediction have only been able to make predictions with error. For example, returning to curve-fitting, suppose the available data are not perfectly collinear but rather appear to be linear plus a Gaussian error.

When we only have uncertain predictive methods to choose from, our choice should be guided by the following generalization of @prp-worked (which is likewise justified by @prp-same and our pragmatic need for prediction):

::: {#prp-best}
A prediction should be made using a predictive method that has---or could have---been most successful in making similar predictions.
:::

What counts as "most successful" will depend on the prediction task at hand, the metric that matters in a particular context. If there is money at stake, for instance, and the loss depends on the magnitude (and potentially the direction) of the error, "most successful" would mean minimizing that loss function. Alternatively, for predictive methods that explicitly acknowledge uncertainty by specifying a probability distribution over potential outcomes, the predicted likelihood provides a general-purpose success metric. Whatever the relevant measure of success is, @prp-best says to use the predictive method shown capable of maximizing it.

What counts as "similar predictions" must also be specified in relation to the prediction task at hand. In @sec-prag, we considered the definition of what counts as "similar predictions" (in @prp-same and @prp-worked) as part of the predictive method in question. But this may not suffice when it comes to comparing the success of predictive methods, as in @prp-best, as opposed to assessing the success of a single method as in @prp-worked. A common set of "similar prediction" tests should be considered as part of the context of the prediction task, rather than something that could be different for each predictive method.^[Should weight the validation tests based on similarity to the task for which it will be used. That may simply mean equal weights (when LOO is appropriate, for example).]

The best predictive method for a given purpose will not necessarily correspond to the true data-generating process. For our linear curve-fitting example, with predicted likelihood as the metric, it probably will: fitting a Gauss Linear model to the data will generally be the most successful predictive method. But suppose the data were actually generated by a sine curve of high frequency and low amplitude relative to the Gaussian noise added to it. Unless we have a very large number of observations, the available data will not be sufficient to distinguish the sine curve from the linear model—and attempting to fit a sine curve to a small data sample will likely result in worse predictions than the simple linear model.

-   Add a numerical sine curve fitting example, in which linear is best (perhaps even without added random noise)

Since our specified aim is making predictions, though, identifying the true data-generating process need not concern us directly.[^18] It may be helpful to consider a more concrete, albeit contrived, example.

### Example: Predicting height by age and sex

Suppose we have data on age, sex, and height for a random sample of those aged 20-74 in a given population.[^2a] And suppose the goal is to predict the height of someone aged 20-74, chosen at random.[^3a] We consider a range of predictive methods, which each involve estimating, by ordinary least squares, a statistical model in which height is specified as a function of age and sex. Leave-one-out (LOO) cross-validation is the natural choice for quantifying predictive performance in this context, but it is shown alongside other standard measures in the table below.[^4a]  The most successful method is the specification ‘*female age age2*’. In other words, projecting height onto a linear combination of gender, age and age squared performs best for predicting height. So that is the model we should use to predict the height of someone aged 20-74 in this population.

[^2a]: And suppose we have no other information to go on. The data for this example is Stata’s “nhanes2d” dataset, downsampled in such a way that each observation in the resulting sample has equal weight, to avoid the complications of accounting for stratified sampling.

[^3a]: Use the (negative) mean squared error is the measure of success.

[^4a]: The LOO R^2^ is calculated by predicting each observation using only the other observations to estimate the model (by OLS, here), then tallying up the squared prediction errors and expressing the average as an R^2^. It has been shown to be asymptotically equivalent to the AIC by @stone1977, explaining their equivalent rankings.)

![Example Comparative Model Selection Measures](example-measures.png)

This selected model is most certainly *not* the true data-generating process---nor is any other parametric model, it is safe to say. But it is the best predictive method we have available.

-   Try model averaging.-   

Now suppose we’re interested in generalizing from this data,^[Here "generalization" is used with reference to [@busemeyer2000].] which is limited to ages 20-74. Suppose we want to use it to predict the height of someone aged 75-85, randomly selected from this population. The leave-one-out prediction measure used above is less suitable for evaluating a method’s ability to predict the heights of other ages because it is less closely analogous to the prediction task of interest. The table below shows the mean squared predictive error for 15 more similar prediction tasks that can be checked with the available data (so here, lower is better). (A) involves predicting heights for ages 73-74 using data on only ages 20-72. Among these 15 alternatives, it is most like the prediction task of interest in terms of how much data it has available to fit the model. (J), predicting ages 64-74 using data for ages 20-63, is most like the desired prediction task in terms of how many years out it is extrapolating, but it differs in that it has 11 fewer years of data with which to fit the model. There are a range of prediction tests between (A) and (J). (K)-(O) are also included as alternative tests of extrapolating 11 years out, though they differ increasingly from the desired prediction task in other respects (both in terms of the predicted age range and the amount of data available to fit the model). It’s not clear which of these alternatives is the best single test of predictive performance akin to extrapolating from ages 20-74 to ages 75-85. But each of them is more closely analogous to the desired prediction task than is the leave-one-out prediction used above.

![Mean Squared Prediction Error for Different Prediction Tasks, by Ages Held Out for Prediction](different-tasks.png)

The specification ‘*female age age2 c.female#(c.age c.age2)*’ turns out to perform best, on average, whether you limit attention to (A) through (J) or also include (K) through (O). The table below summarizes each specification’s performance, alongside the LOO R^2^ from earlier, for comparison.

![LOO R^2^ and Average Mean Squared Prediction Error Across Prediction Task Groups](loo-comparison.png)

Amidst the apparent noise, one can see a general pattern of simpler models performing better with less data to train on (that is, longer held-out periods like in (J)-(O)), and vice versa.[^15a] Indeed, there could be an argument for using a simpler model like ‘*female age2*’ for extrapolating to ages 75-85, based on its superior performance in tests (J)-(O), which each likewise involves extrapolating 11 years out. Or one could argue for the more general model \`*female age age2 age3 female#(age age2)*’ based on its superior performance in tests (F)-(J), which allow training on data closer to the full dataset that will be used to extrapolate in the prediction task of interest. Again, though, ‘*female age age2 c.female#(c.age c.age2)*’ performs best overall across all these tests.[^16a]

[^15a]: The (relatively simple) model selected by LOO R^2^ runs counter to this pattern. (And this disconnect underscores the importance of differentiating between interpolation and extrapolation when assessing a model specification.)

[^16a]: Yet, given the closeness in performance between several of the specifications, one might also argue for model averaging or else presenting predictions using multiple specifications.

There are still judgments to be made, but these judgments can be made based on criteria of clear relevance to the prediction task at hand. In contrast, the choice between Adjusted R^2^ and BIC, for instance, would generally be based on more abstract criteria unconnected to the specific question of interest.

-   Maybe use [@myrvold2002] orbital period chart example

If the data are approximately linear,[^19] a best-fit line will be able to predict well with relatively few known measurements available as inputs. The method of instead, given \_n \_known data points, fitting a polynomial \_p \_of degree \_n – 1 \_will fit the known data exactly but will likely fluctuate wildly up and down in between the known points and thus predict unknown data poorly.[^20] We can reject other alternative hypotheses in the same way, leaving the best-fit line as the most successful predictive method.[^21]

[^19]: To fix ideas, assume the true data-generating process is a linear function plus Gaussian noise.

[^20]: Yet, what can we say about the predictive method that simply prescribes predicting points to be on the polynomial *p*, rather than prescribing a method for fitting a polynomial to the points? This method would have been successful at predicting all *n* points without requiring knowledge of any of them. Well, if you had arrived at this method before knowing of the data and you predicted all n points exactly, I would counsel you to continue using this method. However, if there was no way for you to know of *p* without knowledge of the data, you cannot accurately say that you could have predicted the data with this method. It is misleading to even consider it a complete method if your actual method included deriving *p* from the data. Thus, we can deny this contrivance the status of a successful method without even appealing to a need for further tests of its predictions.

[^21]: The line method is not the only method that would work to produce good predictions of approximately linear data, but it is the one that allows the most robust predictions for a given size of input dataset. Fitting second (or third) degree polynomials to linear data would lead to approximately linear prediction curves if enough points are taken into consideration, so these would also give successful predictions with large known datasets. In that case, any low-order polynomial will give essentially equivalent predictions, so that the choice between them is immaterial (since what matters is the prediction, not isolating a unique method for “confirmation”). But fitting a line to the data will perform better (again, assuming approximately linear data) for few data points—that is, when the difference between methods matters.

## The objections to Reichenbach's pragmatic justification {#sec-induct}

Situate within recent literature: Stanford entry, contrasting with Reichenbach and more recent pragmatic attempt (ML-inspired formulation that doesn't actually fit with ML practice)

## Schurz's justification for induction {#sec-schurz}

::: callout-note
June 2024: Address the critique of "most successful" in that induction paper.
:::

## Induction {#sec-hume}

In his book *The Knowledge Machine*, Michael Strevens provides a compelling account of how science works, a lucid synthesis of decades of insights from history and philosophy of science, albeit with one unsettling limitation. Strevens makes the quandaries of philosophers actually fun to read about, including David Hume’s problem of induction, the lack of a reasoned justification for generalizing from experience. On the stakes of this problem, he quotes Bertrand Russell, who said that if inductive reasoning cannot be justified, “there is no intellectual difference between sanity and insanity” (p. 17) And Russell is not alone. As Strevens puts it, almost all philosophers of science “believe that induction is essential to human existence” (p. 21).[^22] Yet, “there is still no widely accepted justification for induction” (p. 17).

[^22]: “Some believe that Hume’s problem must have a solution—that is, a philosophical argument showing that it is reasonable to suppose that nature is uniform in certain respects... Some believe, like Hume himself, that it has no solution but that we must go on thinking inductively regardless, both in our science and in our everyday lives” (p. 21). Strevens does not make clear to which camp he himself belongs.

Any inductive inference from known information to unknown can be thought of as a prediction, broadly construed, so @prp-worked is a pragmatic justification for induction.

------------------------------------------------------------------------

To sum up, my claim is that the underlying logic of inductive reasoning is, essentially, “use a predictive method that works.” I have provided an *a priori* justification for this principle. And I have shown how it accounts for the three features of the natural human approach to inductive reasoning that Strevens identifies.

\[Brief note re brain as predictive machine, citing a recent article\]

------------------------------------------------------------------------

Still, perhaps you are thinking, “So what?” Perhaps only a philosopher could care whether common-sense attitudes can be grounded in a more foundational rationale. Well, elsewhere I argue that this refined grounding for inductive reasoning does have wide-reaching practical implications for how science should be conducted and interpreted. But for now I would just situate the relevance of these considerations by returning to Strevens’ *The Knowledge Machine* (which you really have to read to appreciate his novel, incisive account of what is unique about modern science and how it originated)*.*

Strevens’ book goes on to argue that the power of modern science stems from how it forces scientists to relentlessly prioritize tedious empirical observations over the aesthetic, religious, or philosophical considerations to which pre-modern students of the natural world dedicated the bulk of their attention and writing. But Strevens never returns to the basic question of how generalizing from all those meticulous observations can be justified in the first place.[^23] Strevens’ readers, despite his forceful presentation of Hume’s problem of induction in the first chapter, will probably have safely forgotten this disturbing quandary by the end of his subsequent thirteen chapters. But the justification of inductive reasoning presented here allows a proper appreciation of why all that careful observation Strevens celebrates matters: by eliminating rival predictive methods, we are left not with a bewildering infinitude of theories yet capable of fitting our still-finite observations but rather, for so many practical purposes, a single relevant predictive method, enabling our ever-expanding technical mastery.

[^23]: Strevens gestures toward “Baconian convergence” but does not attempt to explain why this seems to work so well rather than being defeated by the underdetermination that seems so inevitable when confirmation theory is considered in the abstract.

## Science {#sec-science}

More tentatively, I would like to propose that how theory confirmation works in science could be better understand in terms of predictive methods.

In many cases, especially in the softer sciences, we have no hope of correctly specifying the true model anyhow.

## Statistics {#sec-stats}