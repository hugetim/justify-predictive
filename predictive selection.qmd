---
title: Predictive performance as a basis for model selection
author:
  - name: Tim Huegerich
    acknowledgements: This essay has benefited from the feedback of Sam Merten.

format: 
  html:
    toc: true
    embed-resources: true
theme: default
fig-cap-location: top
bibliography: references.bib
---

# Introduction

Predictive ability is the measure of scientific knowledge. Whereas rationalizing after the fact is notoriously prone to self-deception (no matter how good the “fit”), predicting something previously unknown is a genuine feat, universally acknowledged. So why not use predictive performance as the standard for model selection in statistics?

Put simply, to predict something, one ought to use the method that has (or could have) performed best in making similar predictions.[^1] This principle gives us a model selection criterion for any problem that can be formulated in terms of a prediction. And almost any statistical question can be thought of in terms of predictions. For instance, though we do not usually think of it this way, estimating the average height of a group of people based on a random sample can be thought of as “predicting” the overall average height we would find if we were to literally measure the heights of the whole population. Or one can equivalently ask how best to predict the height of a randomly selected person.

[^1]: This essay has benefited from the feedback of Sam Merten.

### Example: Average height by age and gender

To make things concrete, consider a random sample of 1,482 height measurements for people with ages ranging from 20-74.[^2] The idea is to select the model that could have predicted these measurements most successfully.[^3] Leave-one-out (LOO) cross-validation is the natural choice for quantifying predictive performance in this context.[^4] Among polynomial models, it selects ‘*female age age2*’ as the best specification.[^5] In other words, projecting height onto a linear combination of gender, age and age squared performs best for predicting out-of-sample height. So that is the model we should use to predict the height of someone in this population.

[^2]: Stata’s “nhanes2d” dataset downsampled in such a way that each observation in the resulting sample has equal weight, to avoid the complications of accounting for stratified sampling. Imagine the only available data fields are height, age, and gender.

[^3]: Here we use the (negative) mean squared error as the measure of success, for simplicity. The ideal measure, in the absence of a context-specified penalty function, may be predictive maximum likelihood.

[^4]: The LOO R^2^ is calculated by predicting each observation using only the other observations to estimate the model (by OLS, here), then tallying up the squared prediction errors and expressing the average as an R^2^. (It has been shown to be asymptotically equivalent to the AIC, explaining their equivalent rankings.: [@stone1977].)

[^5]: And it also clearly outperforms the fully general interacted model ‘*i.female##i.age*’.

![Example Comparative Model Selection Measures](example-measures.png)

This selected model is most certainly *not* the true data-generating process—nor is any other parametric model, it is safe to say. Grounding model selection in predictive performance allows us to explicitly acknowledge that reality. Can we also re-frame other forms of statistical reasoning typically premised on pretending we know the true data-generating process?[^6]

[^6]: In current practice, to select (or post-hoc justify) the model specification that we will pretend is true, we typically appeal to AIC/BIC (or Adjusted R^2^), the statistical significance of coefficients, or theory/conventions specific to particular subject areas. We may default to the simplest model that can’t be rejected by a specification test—or the most flexible model with at least 10 observations for each coefficient. Any such convention is helpful for preventing uninhibited, result-driven data mining. But when it comes to justifying a particular convention, either it’s just something “everyone does” (and so needs no explicit justification) or it performs well in Monte Carlo studies in which the true data-generating process is known. The latter justification is arguably no more relevant than the former for real-world data work, in which we know at the outset that none of our candidate models are the true data-generating process. And the Bayesian pretense of assigning prior probabilities to particular data-generating processes being true is no better justified.

Let’s consider statistical significance. Suppose we are interested in whether the average man is taller than the average woman in this population, after controlling for age. (Our selected model calculates the average gender height difference as 13.4 cm.) Standard statistical inference starts from the assumption that the model specification is the true data-generating process, asking how likely it would be to generate data with that large of an average gender difference in height if the true gender coefficient were zero. We can follow an analogous thought process while explicitly acknowledging that our model is merely the most successful known predictive method. If this estimated model were used to simulate (i.e., predict) a new version of the dataset, of the same size and gender/age distribution,[^7] how likely would this simulated data be to exhibit the opposite result, taller women than men on average, when re-estimating the model on the simulated data? The answer turns out to be the same, mathematically, as the p-value for a standard one-sided t-test (\< 0.00025, in this case).[^8] Akin to the conventional interpretation of standard errors, this proposed predictive measure quantifies the strength of the evidence provided by the dataset for the hypothesis that men are taller than women on average.[^9] Stated more simply, its interpretation in this proposed framework is that it predicts the likelihood of an equivalent study failing to replicate.

[^7]: That is, imagine simulating a new height for each of the 1,482 observations in the sample (without changing the gender coefficient to zero as in the standard statistical significance thought experiment). Doing so requires a probability distribution for the residuals, and the answer will depend on what is assumed about that distribution. But if you assume i.i.d. normal errors, the answer corresponds exactly to the ordinary standard error. (Ultimately, we could and should use predictive performance to assess such assumptions about the error distribution, but set that aside for the moment.)

[^8]: Likewise, we can construct a 95% confidence interval in the usual way. It has this “predictive” interpretation: the average gender difference in such a dataset simulated using the best known predictive method has a 95% probability of falling in the interval (12.7 to 14.0 cm, in this case).

[^9]: An analogous approach could be used to quantify how likely it is that a different specification would be chosen using a different random sample for the dataset.

# Causal questions as predictions

It is conventional to contrast “mere prediction” with answering the *causal* questions that are usually of most interest. But we can think about causal questions, too, in terms of predictions. Suppose we want to know the causal impact of some training program on a participant’s earnings. That can be thought of as the difference between their predicted earnings with the training and their predicted earnings without the training, other things equal. That is different from predicting someone’s earnings using all available information, the typical “mere prediction” scenario, but it is a coherent prediction task.[^10] And going through the trouble of formulating a causal impact question as a prediction (of \_some \_kind) gives us a clear standard for model selection.

[^10]: Another way of phrasing it is to say that “mere” prediction concerns observational forecasts: what is going to be observed at such-and-such time? In contrast, causal/counterfactual questions concern interventional forecasts: what *would* be observed if such-and-such were done?

To clarify the distinction between a prediction of causal impact and a “mere prediction” using all available information, suppose we give training participants (as well as the control group) an exit exam known to be correlated with future earnings, the outcome of interest. If we were interested in predicting future earnings for its own sake, we would certainly make use of such exam results to do so. But to address our *causal* question in terms of prediction, we would restrict ourselves to predictive methods using only control variables not themselves causally impacted by the training, so as to measure the full impact of the training on future earnings. Subject to that restriction, predictive performance provides an objective standard for model selection to address causal impact.[^11]

[^11]: For the moment, suppose program participation were randomized. Given a range of candidate model specifications combining participation in the training program with other observable characteristics not themselves causally impacted by the training, the one that best predicts earnings one year later (in the leave-one-out sense) provides the most reliable measurement of the impact of the training program.

For the moment, suppose program participation were randomized. We consider a range of model specifications combining participation in the training program with other observable characteristics not themselves causally impacted by the training. The one that performs best at predicting earnings one year later (in the leave-one-out sense) provides the most reliable measurement of the impact of the training program.

We can handle the case of non-random selection into the training program similarly. That is, suppose participation is causally impacted by prior ability, which itself has a direct causal impact on future earnings. We would then limit ourselves to predictive methods that substitute one or more instruments for program participation (something that causally impacts program participation but does not impact earnings otherwise) when framing the program evaluation question in terms of prediction. That is, we would look for the most predictive reduced-form estimator.[^12]

[^12]: That is, predictive performance can also inform the choice of which “instruments” to include.

# Applying the predictive performance criterion in practice

The idea, again, is that predictive performance can provide an authoritative guideline for model selection, taking the place of the various rules of thumb currently in use. The principle is: “**Use the predictive method that has been most successful at making similar predictions in the past.**” Judgment is still required to apply this principle in practice, of course. But using judgment in applying a unifying principle is very different from choosing among a smorgasbord of incommensurate criteria and rules of thumb.

Still, making decisions in particular cases requires more concretely specifying what is meant by each of the key phrases in the bolded sentence above. For starters, what should count as a **predictive method?**[^13] Suppose, in the context of the height example discussed above, someone secretly ran the fully interacted model on the full dataset but then purported to have arrived at its point estimates by some \_a priori \_theory, one which just so happens to “predict” the average height of a 20 year-old male as 176.4 cm, a 21 year-old male as 176.8 cm, and so on. Such a “method” would achieve that model’s R^2^ of 0.5547 as its apparent LOO R^2^ because it (purportedly) makes those predictions regardless of training dataset. Well, perhaps that would be easily recognized as cheating, but there may be subtler ways to smuggle information from the full dataset into a method’s “predictions” that would be more difficult to recognize.[^14] Pre-registered predictions of unknown data provide a gold standard that cuts through such games. Though they can be difficult, if not impossible, to arrange in practice, we can at least carry out the thought experiment: “Could you really have predicted in that way?”

[^13]: I use “predictive method” to denote something more specific than a model: it also specifies the estimator to be used. A predictive method provides complete instructions for using a “training” dataset to make predictions about observations outside that set.

[^14]: It may even be that selecting a model based on cross-validation always “taints” your subsequent predictions for truly out-of-sample data, to some extent. It would certainly raise concern about overfitting, for instance, to use cross-validation to select from among a million different predictive methods, rather than limiting attention to just a few as I tend to imagine. (In any case, assessing the accuracy of a model chosen by cross-validation requires sequestering a separate "test" set up front, data not to be used for model selection in any way.)

How to quantify which method has been **most successful** in a LOO prediction exercise also requires judgment. In the above example, I used LOO R^2^ (or, equivalently, mean squared predictive error). For “parametric” methods that fully specify a predicted probability distribution, the mean LOO log-likelihood would be a natural choice. Or there may be a context-specific loss function, such as the dollars lost due to the failed prediction, in a financial context.

There is perhaps the most potential for dispute in determining what counts as **similar predictions** relevant to the statistical question of interest. Identical predictions will often not be possible to check with the data already available, and then it may be unclear which predictions are most closely analogous. Still, it should be clear that some pseudo-predictions are more closely analogous to the prediction of interest than others.

### Example: Extrapolating average height for ages 75-85

To illustrate this last issue, consider an extension of the height example discussed above. Suppose we’re interested in extrapolating from this data, which is limited to ages 20-74. Suppose we want to use it to predict the average heights of people aged 75-85. The leave-one-out prediction measure used above is less suitable for evaluating a model specification’s ability to extrapolate the heights of other ages because it is less closely analogous to the prediction task of interest. The table below shows the mean squared predictive error for 15 more closely analogous prediction tasks that can be checked with the available data (so here, lower is better). (A) involves predicting heights for ages 73-74 using data on only ages 20-72. Among these 15 alternatives, it is most like the prediction task of interest in terms of how much data it has available to fit the model. (J), predicting ages 64-74 using data for ages 20-63, is most like the desired prediction task in terms of how many years out it is extrapolating, but it differs in that it has 11 fewer years of data with which to fit the model. There are a range of prediction tests between (A) and (J). (K)-(O) are also included as alternative tests of extrapolating 11 years out, though they differ increasingly from the desired prediction task in other respects (both in terms of the predicted age range and the amount of data available to fit the model). It’s not clear which of these alternatives is the best single test of predictive performance akin to extrapolating from ages 20-74 to ages 75-85. But each of them is more closely analogous to the desired prediction task than is the leave-one-out prediction used above.

![Mean Squared Prediction Error for Different Prediction Tasks, by Ages Held Out for Prediction](different-tasks.png)

The specification ‘*female age age2 c.female#(c.age c.age2)*’ turns out to perform best, on average, whether you limit attention to (A) through (J) or also include (K) through (O). The table below summarizes each specification’s performance, alongside the LOO R^2^ from earlier, for comparison.

![LOO R^2^ and Average Mean Squared Prediction Error Across Prediction Task Groups](loo-comparison.png)

Amidst the apparent noise, one can see a general pattern of simpler models performing better with less data to train on (that is, longer held-out periods like in (J)-(O)), and vice versa.[^15] Indeed, there could be an argument for using a simpler model like ‘*female age2*’ for extrapolating to ages 75-85, based on its superior performance in tests (J)-(O), which each likewise involves extrapolating 11 years out. Or one could argue for the more general model \`*female age age2 age3 female#(age age2)*’ based on its superior performance in tests (F)-(J), which allow training on data closer to the full dataset that will be used to extrapolate in the prediction task of interest. Again, though, ‘*female age age2 c.female#(c.age c.age2)*’ performs best overall across all these tests.[^16]

[^15]: The (relatively simple) model selected by LOO R^2^ runs counter to this pattern. (And this disconnect underscores the importance of differentiating between interpolation and extrapolation when assessing a model specification.)

[^16]: Yet, given the closeness in performance between several of the specifications, one might also argue for model averaging or else presenting predictions using multiple specifications.

There are still judgments to be made, but these judgments can be made based on criteria of clear relevance to the prediction task at hand. In contrast, the choice between Adjusted R^2^ and BIC, for instance, would generally be based on more abstract criteria unconnected to the specific question of interest.